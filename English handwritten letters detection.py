# -*- coding: utf-8 -*-
"""Handwritten_Detection_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LakmUY6GaZc1od7caj2Lhhzc8tJIoGXZ

#EE6350 Artificial Intelligence

## Handwritten Text Recognition

### The purpose of this project is to give a brief idea and a basic approach for offline handwritten text recognition by using segmentation and classification.

## Approach

* **Step1** :  Build a digit(0-9) + A-Z characters classifier using a CNN architecture.
* **Step2** :  Apply character segmentation for the handwritten word image.
* **Step3** :  Classify each segmented letter and then get the final word in the image.

### Handwritten Text Recognition - Data Preparation
   
1. Importing Necessary Libraries
   
2. Setting Up Dataset Download Parameters
   
3. Preparing Kaggle Directories
   
4. Cleaning and Creating Directories
   
5. Creating Symbolic Links
   
6. Downloading and Extracting Datasets
   
7. Completion Message
"""

pip install gradio

import gradio as gr

def greet(Topic):
  return "Handwritten English Letters Detection /  " + Topic + " "

iface = gr.Interface(fn=greet, inputs= "text", outputs= "text")
iface.launch()

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'handwritten-characters:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F53376%2F101598%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240624%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240624T101303Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D164fac58b1a58f353cdc2fb85f2eeb41e0e09f2c69a3ae31939843d8ab5f464574d6cda48c3f175884ff58b91a5f7f3a10fdc55297e31c41480f547fdcfecba38d9233153b46cf517bbb8fe132871abb59fa28abe214feee253c6096f62d0fef5ffc6318dfb67622e0bf4983491ca0354041e694b54355cb0a0458027e62cebf689965ab338b990ffd10cc88322bef4ccf3a6809733e3fc7081444e24ead6b1b151e2775baeeb5d0477ebc54cb0dd9b52899d5b3c613371ae61e1dc1769dfc492e0eaeec7feb490f2389891682739624d7adce09c75ed19d626497a5dd69af4bfd8898302716aa0b88e486187787709ee480aafb0f58e7ac4145fc09562d430d,handwriting-recognition:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F818027%2F1400106%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240624%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240624T101303Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D333d99e5b1de2a36a831ed0b625293f5be9cc7cffd22a789ce132d2c9b3d7d3aca6c324d280a17269a6cd75e2b122ccf4c60050a330a0f650a53d6bb19e2561b55ee4d071d22c24d274a50d60ef531f723576bcf1860072da247291da5a788d12cec752d5d8b7cc7e5a3c52ac99db8dbf1d7355a1176a76fc42aee6d835fb5b791fbeac957f5218366721db3df0c3150301c84487e6dbafd06108baa1b10486c41b46dbc4b6e9c9d287c7ec71492589fd7c06ce1769c77c1b2bd0320d2a8bb3477e3f9747fc6bfce815c5dd32acdc418922e2ea12dbe65cd6a892547285c14bda3e5b29f92f5c36f35896be52d0fd53154e574db30d9f37b895432a5519b5269'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

# Unmount and remove the existing input directory, if any
!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)

# Create new input and working directories with full permissions
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)


# Create symbolic links for the input and working directories
try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

# Loop through each data source in the DATA_SOURCE_MAPPING
for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
      # Open the URL and create a temporary file
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)

                # Extract the downloaded file if it is a zip or tar file
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

!pip install imutils

"""## **Importing Libraries for Data Handling and Model Building**

In this section, we import a comprehensive set of libraries necessary for handling data and building our machine learning model for handwritten text recognition. These libraries provide tools for data manipulation, preprocessing, visualization, and deep learning model construction.
"""

import numpy as np
import pandas as pd
from keras.preprocessing.image import ImageDataGenerator
import os
import random
import cv2
import imutils
import random
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelBinarizer

# Instead of keras.utils import np_utils use the below import
from tensorflow.keras.utils import to_categorical
from keras.models import Sequential
from keras import optimizers
from sklearn.preprocessing import LabelBinarizer
from keras import backend as K
from keras.layers import Dense, Activation, Flatten, Dense,MaxPooling2D, Dropout
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization

"""## **Preparing the Training Data**

In this section, we set up the directory path for our training dataset and preprocess the images to make them ready for model training. This involves reading images, resizing them to a uniform size, and filtering out specific non-character images. The preprocessed images are then stored in a list along with their respective labels.
"""

dir = "../input/handwritten-characters/Train/"
train_data = []
img_size = 32
non_chars = ["#","$","&","@"]
for i in os.listdir(dir):
    if i in non_chars:
        continue
    count = 0
    sub_directory = os.path.join(dir,i)
    for j in os.listdir(sub_directory):
        count+=1
        if count > 4000:
            break
        img = cv2.imread(os.path.join(sub_directory,j),0)
        img = cv2.resize(img,(img_size,img_size))
        train_data.append([img,i])

len(train_data)

"""## **Preparing the Validation Data**

In this section, we set up the directory path for our validation dataset and preprocess the images in a similar manner to the training dataset. This process includes reading and resizing the images, filtering out specific non-character images, and storing the preprocessed images along with their corresponding labels. Validation data helps in evaluating the model's performance on unseen data during the training phase.
"""

val_dir = "../input/handwritten-characters/Validation/"
val_data = []
img_size = 32
for i in os.listdir(val_dir):
    if i in non_chars:
        continue
    count = 0
    sub_directory = os.path.join(val_dir,i)
    for j in os.listdir(sub_directory):
        count+=1
        if count > 1000:
            break
        img = cv2.imread(os.path.join(sub_directory,j),0)
        img = cv2.resize(img,(img_size,img_size))
        val_data.append([img,i])

len(val_data)

random.shuffle(train_data)
random.shuffle(val_data)

""" ## **Separating Features and Labels for Training Data**


This section focuses on splitting the preprocessed training data into two distinct lists: one for the features (images) and another for the labels (characters). This separation is essential for training the model, as it requires the input features to learn patterns and the corresponding labels to understand the classification targets.
"""

train_X = []
train_Y = []
for features,label in train_data:
    train_X.append(features)
    train_Y.append(label)

"""## **Separating Features and Labels for Validation Data**

In this section, the validation data is split into two separate lists: one for the features (images) and one for the labels (characters). This separation is crucial for the subsequent evaluation of the model's performance on unseen data during the training phase.
"""

val_X = []
val_Y = []
for features,label in val_data:
    val_X.append(features)
    val_Y.append(label)

"""## **Encoding Labels for Training and Validation Data**

In this section, the labels for both the training and validation datasets are encoded using LabelBinarizer from scikit-learn. This transformation converts categorical labels (characters) into binary vectors, which are essential for training and evaluating classification models.
"""

LB = LabelBinarizer()
train_Y = LB.fit_transform(train_Y)
val_Y = LB.fit_transform(val_Y)

"""## **Preparing Training Data for Model Input**

In this section, the training data (train_X and train_Y) is prepared for input into the deep learning model. This involves scaling the pixel values of images and reshaping them to conform to the expected input shape of the model.
"""

train_X = np.array(train_X)/255.0
train_X = train_X.reshape(-1,32,32,1)
train_Y = np.array(train_Y)

"""## **Preparing Validation Data for Model Evaluation**

In this section, the validation data (val_X and val_Y) is prepared for evaluation with the trained deep learning model. Similar to the training data preparation, this involves scaling the pixel values of images and reshaping them to match the expected input shape of the model.
"""

val_X = np.array(val_X)/255.0
val_X = val_X.reshape(-1,32,32,1)
val_Y = np.array(val_Y)

print(train_X.shape,val_X.shape)

print(train_Y.shape,val_Y.shape)

"""## **Building a Convolutional Neural Network (CNN) for Handwritten Character Recognition**

This section outlines the construction of a Convolutional Neural Network (CNN) designed to recognize handwritten characters from images. CNNs are particularly effective for image classification tasks due to their ability to automatically learn hierarchical representations of visual data.
"""

# Create a Sequential model
model = Sequential()

# Add the first convolutional layer with 32 filters, a 3x3 kernel, ReLU activation, and same padding
# This layer takes input of shape (32, 32, 1)
model.add(Conv2D(32, (3, 3), padding="same", activation='relu', input_shape=(32, 32, 1)))

# Add a max pooling layer with a 2x2 pool size
model.add(MaxPooling2D(pool_size=(2, 2)))

# Add the second convolutional layer with 64 filters, a 3x3 kernel, and ReLU activation
model.add(Conv2D(64, (3, 3), activation='relu'))

# Add another max pooling layer with a 2x2 pool size
model.add(MaxPooling2D(pool_size=(2, 2)))

# Add the third convolutional layer with 128 filters, a 3x3 kernel, and ReLU activation
model.add(Conv2D(128, (3, 3), activation='relu'))

# Add another max pooling layer with a 2x2 pool size
model.add(MaxPooling2D(pool_size=(2, 2)))

# Add a dropout layer with a 25% dropout rate to prevent overfitting
model.add(Dropout(0.25))

# Flatten the output from the convolutional layers to create a 1D feature vector
model.add(Flatten())

# Add a fully connected (dense) layer with 128 units and ReLU activation
model.add(Dense(128, activation='relu'))

# Add another dropout layer with a 20% dropout rate to prevent overfitting
model.add(Dropout(0.2))

# Add the output layer with 35 units (for 35 classes) and softmax activation for classification
model.add(Dense(35, activation='softmax'))

model.summary()

"""## **Compiling and Training the CNN Model for Handwritten Character Recognition**


This section covers compiling the CNN model with specified loss function, optimizer, and metrics, followed by training the model using training and validation datasets.
"""

model.compile(loss='categorical_crossentropy', optimizer="adam",metrics=['accuracy'])

history = model.fit(train_X,train_Y, epochs=5, batch_size=32, validation_data = (val_X, val_Y),  verbose=1)

"""## **Visualizing Training Metrics: Accuracy and Loss**

After training the CNN model, it's essential to visualize how the training and validation metrics (accuracy and loss) evolve over epochs. This helps in understanding the model's performance and identifying any potential issues such as overfitting.

**Training Accuracy vs Validation Accuracy**
"""

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Training Accuracy vs Validation Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""**Training Loss vs Validation Loss**"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Training Loss vs Validation Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""## Recognition and Post-Processing
1. The sort contours function is used to get the correct order of individual characters for correct output extraction. In this case for extracting a single word, a left to right sorting of individual characters is needed.
2. The get letters function fetches the list of letters and get word function gets the individual word.
"""

# Define functions

# Function to sort contours based on the specified method
def sort_contours(cnts, method="left-to-right"):

  # Initialize reverse flag and index
    reverse = False
    i = 0

    # Set reverse flag for right-to-left or bottom-to-top sorting
    if method == "right-to-left" or method == "bottom-to-top":
        reverse = True

        # Set index for top-to-bottom or bottom-to-top sorting
    if method == "top-to-bottom" or method == "bottom-to-top":
        i = 1

        # Get bounding boxes for each contour
    boundingBoxes = [cv2.boundingRect(c) for c in cnts]

     # Sort contours and bounding boxes based on the specified method
    (cnts, boundingBoxes) = zip(*sorted(zip(cnts, boundingBoxes),
                                        key=lambda b: b[1][i], reverse=reverse))

    # Return sorted contours and bounding boxes
    return (cnts, boundingBoxes)

# Function to get letters from an image
def get_letters(img_path, threshold_level=127, dilate_iter=2):
    # Initialize an empty list to store detected letters
    letters = []

    # Read the input image
    image = cv2.imread(img_path)

    # Convert the image to grayscale
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Apply binary inverse thresholding to the grayscale image
    ret, thresh1 = cv2.threshold(gray, threshold_level, 255, cv2.THRESH_BINARY_INV)

    # Dilate the thresholded image to fill gaps
    dilated = cv2.dilate(thresh1, None, iterations=dilate_iter)

    # Find contours in the dilated image
    cnts = cv2.findContours(dilated.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts = imutils.grab_contours(cnts)

    # Sort contours from left to right
    cnts = sort_contours(cnts, method="left-to-right")[0]

    # Loop over the contours
    for c in cnts:
        # Ignore small contours to reduce noise
        if cv2.contourArea(c) > 10:
            # Get the bounding box for each contour
            (x, y, w, h) = cv2.boundingRect(c)

            # Draw a rectangle around the detected letter on the image
            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)

            # Extract the region of interest (ROI) for the detected letter
            roi = gray[y:y + h, x:x + w]

            # Apply binary inverse thresholding to the ROI
            thresh = cv2.threshold(roi, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]

            # Resize the thresholded ROI to 32x32 pixels
            thresh = cv2.resize(thresh, (32, 32), interpolation=cv2.INTER_CUBIC)

            # Normalize the resized ROI
            thresh = thresh.astype("float32") / 255.0

            # Expand dimensions to match the input shape expected by the model
            thresh = np.expand_dims(thresh, axis=-1)
            thresh = thresh.reshape(1, 32, 32, 1)

            # Predict the letter using the model
            ypred = model.predict(thresh)

            # Inverse transform the predicted label to get the letter
            ypred = LB.inverse_transform(ypred)
            [x] = ypred

            # Append the detected letter to the list
            letters.append(x)

    # Return the list of detected letters and the image with rectangles
    return letters, image

def get_word(letters):
    return "".join(letters)

letter,image = get_letters("../input/handwriting-recognition/train_v2/train/TRAIN_00003.jpg")
word = get_word(letter)
print(word)
plt.imshow(image)

letter,image = get_letters("../input/handwriting-recognition/train_v2/train/TRAIN_00023.jpg")
word = get_word(letter)
print(word)
plt.imshow(image)

letter,image = get_letters("../input/handwriting-recognition/train_v2/train/TRAIN_00030.jpg")
word = get_word(letter)
print(word)
plt.imshow(image)

letter,image = get_letters("../input/handwriting-recognition/validation_v2/validation/VALIDATION_0005.jpg")
word = get_word(letter)
print(word)
plt.imshow(image)

pip install gradio tensorflow numpy scikit-learn opencv-python imutils

!pip install gradio



import numpy as np
import cv2
import imutils
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense
from sklearn.preprocessing import LabelBinarizer
import tensorflow as tf
import gradio as gr

# Define your model architecture
model = Sequential()
model.add(Conv2D(32, (3, 3), padding="same", activation='relu', input_shape=(32, 32, 1)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(36, activation='softmax'))  # Adjust output layer to match number of labels
model.compile(loss='categorical_crossentropy', optimizer="adam", metrics=['accuracy'])

# Replace this with the actual path to your model weights file
model_weights_path = "path_to_your_model_weights.h5"

# Load your model weights
try:
    model.load_weights(model_weights_path)
except Exception as e:
    print(f"Error loading model weights: {e}")

# Define the LabelBinarizer and fit it with your labels
LB = LabelBinarizer()
# Define your labels (replace with your actual labels)
your_labels = list("ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789")
# Fit the LabelBinarizer with your labels
LB.fit(your_labels)

# Define your function to process image and predict letters
def sort_contours(cnts, method="left-to-right"):
    reverse = False
    i = 0
    if method == "right-to-left" or method == "bottom-to-top":
        reverse = True
    if method == "top-to-bottom" or method == "bottom-to-top":
        i = 1
    boundingBoxes = [cv2.boundingRect(c) for c in cnts]
    (cnts, boundingBoxes) = zip(*sorted(zip(cnts, boundingBoxes), key=lambda b: b[1][i], reverse=reverse))
    return (cnts, boundingBoxes)

def get_letters(image):
    letters = []
    try:
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        ret, thresh1 = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)
        dilated = cv2.dilate(thresh1, None, iterations=2)
        cnts = cv2.findContours(dilated.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        cnts = imutils.grab_contours(cnts)
        cnts = sort_contours(cnts, method="left-to-right")[0]

        for c in cnts:
            if cv2.contourArea(c) > 10:
                (x, y, w, h) = cv2.boundingRect(c)
                roi = gray[y:y + h, x:x + w]
                thresh = cv2.threshold(roi, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]
                thresh = cv2.resize(thresh, (32, 32), interpolation=cv2.INTER_CUBIC)
                thresh = thresh.astype("float32") / 255.0
                thresh = np.expand_dims(thresh, axis=-1)
                thresh = thresh.reshape(1, 32, 32, 1)
                ypred = model.predict(thresh)
                ypred = LB.inverse_transform(ypred)
                [x] = ypred
                letters.append(x)
    except Exception as e:
        print(f"Error processing letters: {e}")

    return letters

def get_word(letters):
    return "".join(letters)

def recognize_handwriting(image):
    letters = get_letters(image)
    word = get_word(letters)
    return word

# Create Gradio interface
inputs = gr.Image(image_mode='RGB')
outputs = gr.Textbox(label="Recognized Text")

gr.Interface(fn=recognize_handwriting, inputs=inputs, outputs=outputs).launch()

"""##**Setting Up a Flask App for Handwritten Text Recognition with Tesseract**

This Flask application serves as an API endpoint to perform handwritten text recognition using Tesseract OCR. It allows users to upload an image file, processes it using Tesseract, and returns the extracted text.

# **Handwritten Text Detection Project Summary**

This project focuses on developing a deep learning model for handwritten text detection and recognition using convolutional neural networks (CNNs). The main steps of the project are as follows:

1. **Data Preparation**:

The dataset used for training and validation consists of handwritten characters, which are downloaded and extracted from specified URLs. The images are resized to a standard size of 32x32 pixels and labeled appropriately.

2. **Data Augmentation and Preprocessing:**

The data is shuffled and split into training and validation sets. Labels are binarized using a LabelBinarizer. The image pixel values are normalized by scaling them to the range [0, 1].

3. **Model Building:**

A CNN model is constructed using Keras. The architecture includes multiple convolutional layers followed by max-pooling layers, dropout layers for regularization, and dense layers for final classification. The model is compiled with categorical cross-entropy loss and the Adam optimizer.

4. **Training:**

The model is trained on the training dataset for 50 epochs with a batch size of 32. The training accuracy and loss, as well as the validation accuracy and loss, are plotted to visualize the model's performance over epochs.

5. **Prediction Functionality:**

Functions are implemented to extract letters from images by detecting contours, preprocessing the image, and predicting the character using the trained model. The predicted letters are then combined to form words.

6. **Web Application for Image Upload and Text Recognition:**

A Flask web application is developed to provide a user-friendly interface for uploading images and displaying the recognized text. The application uses pytesseract for OCR as a secondary method for text recognition.
"""